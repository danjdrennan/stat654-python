{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>STAT 654: Statistical Computing with R and Python</h1>\n",
    "<h2>Bayesian Computations: Gibbs Samplers</h2>\n",
    "<strong>\n",
    "Daniel Drennan<br>\n",
    "Professor Sharmistha Guha<br><br>\n",
    "Department of Statistics<br>\n",
    "Texas A&M University<br>\n",
    "College Station, TX, USA<br><br>\n",
    "Spring 2022<br>\n",
    "</strong>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook covers the implementation of Gibbs samplers to sample from the posterior distribution in a Bayesian statistical\n",
    "model. It assumes some background on Bayesian computations from the class lectures and course materials, but recaps some of \n",
    "that material here for completeness. After the review, an example of sampling from a t distribution illustrates how to set up\n",
    "a Bayesian statistical model, how to evaluate sampler performance, and how to summarize inference using the posterior\n",
    "distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Suppose $y_1, \\ldots, y_n | \\theta \\overset{iid}{\\sim} f(\\cdot | \\theta)$ for density (or mass) function $f$\n",
    "parameterized by a vector $\\theta$. In classical statistics, we consider $\\theta$ to be a fixed but unknown quantity to be\n",
    "estimated from a random sample using a likelihood function $L(\\theta) = \\prod_{i=1}^{n} f(y_i | \\theta)$. This is known as\n",
    "likelihood-based inference, where our estimates for $\\theta$ are the maximum likelihood estimators $\\hat{\\theta}_{\\text{MLE}}$.\n",
    "\n",
    "In a Bayesian analysis, we assume $\\theta$ is a random variable following a prior distribution $\\pi$. Using Bayes' rule, \n",
    "we can compute a posterior distribution as\n",
    "\\begin{equation}\n",
    "    \\pi(\\theta | y_1, \\ldots, y_n) = \\frac{\n",
    "        \\prod_{i=1}^{n}f(y_i | \\theta) \\pi(\\theta)\n",
    "        }{\n",
    "            \\int_{0}^{\\infty}\\prod_{i=1}^{n}f(y_i | \\theta) \\pi(\\theta) d\\theta.\n",
    "        }\n",
    "        \\tag{1}\n",
    "\\end{equation}\n",
    "The posterior distribution combines all of the information contained in the prior $\\pi$ and the likelihood $L$, and is\n",
    "offers a principled way to update our uncertainty about $\\theta$. Once a posterior distribution is obtained, it can be\n",
    "used to generate credible sets and summary statistics from the distribution, such as estimates of the mean and variance\n",
    "of the distribution. To perform a Bayesian analysis, we must be able to calculate the integral in $(1)$ either analytically\n",
    "or numerically. For a small class of models, called conjugate models, this is trivially easy to do. However, the integral\n",
    "is not generally easy to compute for complex models. In the non-conjugate models, summarizing $\\pi(\\theta | y)$ requires\n",
    "drawing random samples from the posterior distribution and then computing parameters such as the mean, variance, and skew\n",
    "using those samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain Monte Carlo\n",
    "\n",
    "Markov chain Monte carlo (MCMC) is a rich theory which is applicable to this sampling problem, and the Gibbs sampling algorithm\n",
    "is method for sampling from posterior distributions. The Gibbs sampler is useful in problems where we can write hierarchical\n",
    "models on the parameter space. For a joint probability distribution $\\pi(u, v)$, two hierarchical models are \n",
    "$\\pi(u, v) = \\pi(u | v) \\pi(v) = \\pi(v | u) \\pi(u)$ (notice these are all equal!). Our goal is to draw samples from $\\pi(u, v)$.\n",
    "If we can write analytic expressions for $\\pi(u | v)$ and $\\pi(v | u)$, we can use a Gibbs sampler to draw approximate\n",
    "samples from $\\pi(u, v)$. \n",
    "\n",
    "The general approach to running a Gibbs sampler is as follows:\n",
    "\n",
    "* Write the conditional distributions $\\pi(u | v)$ and $\\pi(v | u)$\n",
    "\n",
    "* Initialize $u^{(0)}$ and $v^{(0)}$ and any values in the domains of their distributions\n",
    "\n",
    "* For $t = 1, \\ldots, M$,\n",
    "\n",
    "    - Sample $u^{(t)} \\sim \\pi(\\cdot | v^{(t-1)})$\n",
    "\n",
    "    - Sample $v^{(t)} \\sim \\pi(\\cdot | u^{(t)})$\n",
    "\n",
    "The samples are called approximate samples because (1) they are not independent and (2) the sampler must be run for a\n",
    "sufficiently long period before it returns samples approximating the target distribution $\\pi(u, v)$. We want iid draws\n",
    "from the target distribution, which is also referred to as the stationary distribution, but the sampler does not immediately\n",
    "give us these. To overcome this, we can *burn in* to the Markov chain to reach samples from the stationary distribution\n",
    "(solving problem 2) and *thin* the chain (solving problem 1) to make the samples independent.\n",
    "\n",
    "To make the last point concrete, suppose we know it takes 10,000 samples to reach the target distribution and that\n",
    "$$\n",
    "    \\text{Corr}\\big[u^{(t)}, u^{(t + s)}\\big] =\n",
    "    \\begin{cases}\n",
    "        1 &\\text{if}\\quad t - s = 0, \\\\\n",
    "        \\rho(t - s) &\\text{if}\\quad |t - s| < 4, \\\\\n",
    "        0 &\\text{if}\\quad |t - s| \\geqslant 4.\n",
    "    \\end{cases}\n",
    "$$\n",
    "Above, $\\rho(t - s)$ is any decreasing, nonnegative function in the real numbers which is bounded above by one. \n",
    "This is a formal way of saying the correlation between variables is nonzero for any choice of samples within four timesteps\n",
    "of one another but points from four or more timesteps apart have zero correlation and are effectively \"independent\". In this framework, suppose we want to obtain $N$ samples from the target distribution $\\pi(u, v)$. This will mean drawing \n",
    "$M = 10000 + 4N$ total samples from the sampler. We will drop the first 10,000 samples as the burn-in phase, then take every\n",
    "fourth sample from the remaining chain as our independent samples from the target posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines for a Bayesian Statistical Model\n",
    "\n",
    "In their book *Bayesian Data Analysis*, Gelman et al. recommend the following steps to fitting a Bayesian statistical model:\n",
    "\n",
    "1. Setting up a full probability model for the data and parameters in the model using an appropriate likelihood function for\n",
    "the data and a prior which captures any available knowledge about the underlying scientific problem.\n",
    "\n",
    "2. Conditioning on observed data to calculate and interpret a posterior distribution.\n",
    "\n",
    "3. Evaluating the posterior distribution using diagnostic plots to assess model fit, and conclusions that can be drawn from\n",
    "the posterior distribution.\n",
    "\n",
    "This book and other introductory literature are listed in the references at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Sampling From a T Distribution\n",
    "\n",
    "The following example is similar to the one shown in Section 3.3 of *Bayesian Data Analysis*\n",
    "([pdf here](http://www.stat.columbia.edu/~gelman/book/)). The density for a $t$ distribution with $\\nu$ degrees of freedom,\n",
    "location $\\mu \\in \\mathbb{R}$, and scale $\\sigma > 0$ is\n",
    "\\begin{equation}\n",
    "    f(t | \\nu, \\mu, \\sigma) =\n",
    "    \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\sigma\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}\\left[\n",
    "        1 + \\frac{\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}{\\nu}\n",
    "    \\right]^{-\\frac{\\nu + 1}{2}}.\n",
    "    \\tag{2}\n",
    "\\end{equation}\n",
    "It turns out that the distribution can be written hierarchically as\n",
    "\\begin{align*}\n",
    "    X | \\tau &\\sim N(\\mu, \\tau^{-1}\\sigma^2), \\\\\n",
    "    \\tau &\\sim \\text{Gamma}\\left(\\frac{\\nu}{2}, \\frac{\\nu}{2}\\right),\n",
    "\\end{align*}\n",
    "where\n",
    "$$\n",
    "    f(\\tau | \\frac{\\nu}{2}, \\frac{\\nu}{2}) = \n",
    "    \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\frac{\\nu}{2})} \\tau^{\\nu/2 - 1} e^{-\\frac{\\nu\\tau}{2}}\n",
    "$$\n",
    "is the density of a gamma distribution. The density in $(2)$ is obtained by marginalizing $X | \\tau$ over $\\tau$:\n",
    "\\begin{align*}\n",
    "    \\int_{0}^{\\infty} N(\\mu, \\tau^{-1}\\sigma^2) \\text{Gamma}\\left(\\frac{\\nu}{2}, \\frac{\\nu}{2}\\right) d\\tau\n",
    "    &= \n",
    "        \\int_{0}^{\\infty} \\frac{\\sqrt{\\tau}}{\\sqrt{2\\pi\\sigma^2}}\n",
    "        e^{-\\frac{\\tau}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\n",
    "        \\frac{(\\frac{\\nu}{2})^{\\frac{\\nu}{2}}}{\\Gamma(\\frac{\\nu}{2})} \n",
    "        \\tau^{\\frac{\\nu}{2} - 1} e^{-\\frac{\\nu\\tau}{2}} d\\tau \\\\\n",
    "    &=\n",
    "        \\frac{(\\frac{\\nu}{2})^{\\frac{\\nu}{2}}}{\\sigma\\sqrt{2\\pi}\\Gamma(\\frac{\\nu}{2})} \\int_{0}^{\\infty}\n",
    "        \\tau^{\\frac{\\nu-1}{2}} e^{-\\frac{\\tau}{2}\\left[\n",
    "            \\nu + \\left(\\frac{x - \\mu}{\\sigma}\\right)^2\n",
    "        \\right]} d\\tau \\\\\n",
    "    &= I.\n",
    "\\end{align*}\n",
    "After a change of variables, the integral is a gamma function. Let $y = (x - \\mu) / \\sigma$ and make the\n",
    "substitution $2u = \\tau(\\nu + y^2)$ so $2du = (\\nu + y^2)d\\tau$. Then\n",
    "\\begin{align*}\n",
    "    I\n",
    "    &= \n",
    "        \\frac{(\\frac{\\nu}{2})^{\\frac{\\nu}{2}}}{\\sigma\\sqrt{2\\pi}\\Gamma(\\frac{\\nu}{2})} \\int_{0}^{\\infty}\n",
    "        \\left(\\frac{2u}{\\nu + y^2}\\right)^{\\frac{\\nu-1}{2}} e^{-u} \\frac{2du}{\\nu + y^2} \\\\\n",
    "    &=\n",
    "        \\frac{(\\frac{\\nu}{2})^{\\frac{\\nu}{2}}}{\\sigma\\sqrt{2\\pi}\\Gamma(\\frac{\\nu}{2})}\n",
    "        \\left(\\frac{2}{\\nu + y^2}\\right)^{\\frac{\\nu + 1}{2}}\n",
    "        \\underset{\\Gamma(\\frac{\\nu + 1}{2})}{\\underbrace{\n",
    "            \\int_{0}^{\\infty} u^{\\frac{\\nu - 1}{2}} e^{-u} du\n",
    "        }} \\\\\n",
    "    &=\n",
    "    \\frac{(\\frac{\\nu}{2})^{\\frac{\\nu}{2}}}{\\sigma\\sqrt{2\\pi}\\Gamma(\\frac{\\nu}{2})}\n",
    "    2^{\\frac{\\nu + 1}{2}}\\left(\\nu + y^2\\right)^{-\\frac{\\nu + 1}{2}}\n",
    "    \\Gamma\\left(\\frac{\\nu+1}{2}\\right) \\\\\n",
    "    &=\n",
    "        \\frac{\n",
    "            (\\frac{\\nu}{2})^{\\frac{\\nu}{2} - \\frac{\\nu+1}{2}}\\Gamma(\\frac{\\nu+1}{2})\n",
    "        }{\n",
    "            \\sigma\\sqrt{2\\pi}\\Gamma(\\frac{\\nu}{2})\n",
    "        }\\left(1 + \\frac{y^2}{2}\\right)^{-\\frac{\\nu + 1}{2}}\\\\\n",
    "    &=\n",
    "        \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\sigma\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}\\left[\n",
    "            1 + \\frac{\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}{\\nu}\n",
    "        \\right]^{-\\frac{\\nu + 1}{2}}.\n",
    "\\end{align*}\n",
    "I've shown almost every algebraic step and verified they are correct. If any step does not make sense, feel free to\n",
    "contact me and I will be happy to explain it. I highly recommend working through this derivation with a pen\n",
    "and paper yourself. At the start, write both densities in the hierarchical model and the target density. Then\n",
    "make sure each step in this derivation makes sense."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbf4d2edc3960937e4167eae6bb30e657db03bac26e45aa5902de594be292864"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('stats')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
